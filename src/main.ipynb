{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: mxnet in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (from mxnet) (0.8.4)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (from mxnet) (2.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (from mxnet) (1.25.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (from requests<3,>=2.20.0->mxnet) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (from requests<3,>=2.20.0->mxnet) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (from requests<3,>=2.20.0->mxnet) (2.0.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (from requests<3,>=2.20.0->mxnet) (3.4)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: gluonnlp==0.8.0 in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (0.8.0)\n",
      "Requirement already satisfied: numpy in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (from gluonnlp==0.8.0) (1.25.0)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: tqdm in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (4.65.0)\n",
      "Requirement already satisfied: pandas in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (from pandas) (1.25.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: sentencepiece in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (0.1.97)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (4.26.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: filelock in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (from transformers) (1.25.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (from requests->transformers) (2.0.3)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: torch in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/andrew/opt/anaconda3/envs/huggingFace/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting kobert_tokenizer\n",
      "  Cloning https://github.com/SKTBrain/KoBERT.git to /private/var/folders/0r/6ncbx7693d96pr3xr0xpgt8w0000gn/T/pip-install-k4smajda/kobert-tokenizer_3840534529cd4766aa02766e2dfc19bd\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /private/var/folders/0r/6ncbx7693d96pr3xr0xpgt8w0000gn/T/pip-install-k4smajda/kobert-tokenizer_3840534529cd4766aa02766e2dfc19bd\n",
      "  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet\n",
    "!pip install gluonnlp==0.8.0\n",
    "!pip install tqdm pandas\n",
    "!pip install sentencepiece\n",
    "!pip install transformers\n",
    "%pip install torch\n",
    "%pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['▁한국', '어', '▁모델', '을', '▁공유', '합니다']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "tokenizer.tokenize('한국어 모델을 공유합니다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.0225, -0.1550,  0.2685,  ..., -0.0770, -0.2871, -0.2132],\n",
       "         [-0.1110, -0.1704,  0.1825,  ..., -0.1207,  0.0353, -0.6261],\n",
       "         [ 0.2048,  0.0701,  0.3336,  ..., -0.2434, -0.3100, -0.1624],\n",
       "         ...,\n",
       "         [ 0.1568,  0.1079,  0.1528,  ...,  0.0818, -0.0080, -0.0747],\n",
       "         [ 0.1172, -0.3354,  0.2018,  ..., -0.3257, -0.0151, -0.1935],\n",
       "         [ 0.1812, -0.0182,  0.4336,  ...,  0.0384, -0.6457,  0.1102]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-2.9793e-02,  1.6266e-02, -7.2902e-02,  4.1818e-02, -4.5524e-02,\n",
       "          8.7149e-01, -9.1086e-01,  1.8585e-02,  2.8030e-02, -6.0850e-03,\n",
       "          3.4346e-01, -2.4851e-02, -7.7032e-02,  8.1171e-02, -1.2790e-02,\n",
       "          1.7186e-01, -9.7010e-01,  3.8843e-02,  9.2405e-02,  3.4693e-02,\n",
       "          6.6719e-02, -3.9519e-02,  1.5015e-01, -3.3370e-01,  8.4689e-02,\n",
       "          2.8923e-01, -3.6153e-01, -7.2763e-02,  3.0231e-01, -2.2095e-01,\n",
       "          9.5393e-01, -5.6820e-01, -1.1021e-01,  1.3042e-02, -9.0942e-01,\n",
       "         -6.2711e-01,  4.4016e-02, -5.7798e-02, -8.1763e-01,  2.4504e-02,\n",
       "          3.6985e-02,  7.3949e-02,  4.9737e-02,  9.1739e-01, -5.4457e-02,\n",
       "         -2.3722e-02, -7.1425e-01, -5.7928e-02,  5.6435e-01, -1.0355e-01,\n",
       "         -6.7448e-01,  7.6332e-01,  2.7484e-01,  6.0115e-04, -5.0470e-02,\n",
       "          4.9817e-01,  5.0658e-03,  9.6774e-01, -1.5067e-01, -2.0345e-02,\n",
       "          5.1041e-02, -8.3371e-02, -1.9951e-01, -9.4282e-01,  2.0008e-02,\n",
       "          1.3271e-02,  4.3363e-02,  1.9775e-02, -3.7347e-02, -5.5340e-02,\n",
       "         -2.8074e-01, -1.5025e-02, -2.9699e-01, -1.7854e-02,  3.6887e-01,\n",
       "          3.1139e-03, -9.4473e-03, -1.2012e-02, -1.9135e-01,  4.3891e-02,\n",
       "          3.1103e-02, -6.6376e-01,  2.9822e-01, -5.7024e-01,  9.0724e-03,\n",
       "         -2.6440e-02, -1.3372e-02, -5.5393e-02,  8.8606e-01, -2.8001e-01,\n",
       "         -6.8379e-02,  1.7999e-02,  7.9144e-04, -3.8672e-02, -6.7790e-01,\n",
       "          3.9622e-02,  6.9854e-02, -1.5857e-02,  1.0881e-01,  4.9373e-01,\n",
       "          3.7413e-02, -6.4370e-02,  4.4721e-02, -5.7142e-02, -7.1991e-01,\n",
       "         -3.3330e-02, -3.0731e-02,  4.4470e-02,  5.7956e-02,  2.5403e-03,\n",
       "          2.6738e-02,  4.1276e-02,  9.9002e-01,  6.0084e-02, -1.4370e-01,\n",
       "         -7.2320e-03,  2.6570e-02,  3.9624e-02,  4.7381e-02, -3.1334e-02,\n",
       "          1.3643e-01,  1.5104e-02, -9.7163e-01, -3.0507e-01,  7.7410e-01,\n",
       "          1.5100e-02, -8.8990e-01,  7.0849e-02, -3.5173e-02, -4.8958e-03,\n",
       "          1.4135e-01,  2.1828e-02,  1.0297e-01,  1.9503e-02,  5.6863e-02,\n",
       "          8.0710e-02, -9.5318e-01,  4.5783e-02, -1.4798e-01, -5.1794e-02,\n",
       "         -3.6444e-02,  2.9090e-02,  2.3832e-01,  9.7074e-01, -2.3041e-02,\n",
       "          4.2023e-02, -1.2452e-02, -1.0679e-02,  2.3775e-03, -1.1809e-03,\n",
       "         -6.5365e-01, -7.8735e-02, -1.5162e-01, -5.3787e-02, -1.8731e-02,\n",
       "          5.6081e-02,  1.1355e-01, -7.7183e-03,  5.0778e-02,  2.7885e-01,\n",
       "         -1.4675e-02, -3.1724e-03,  2.0963e-02,  7.0389e-01, -1.5388e-02,\n",
       "          3.2871e-02,  6.6479e-02,  9.8693e-02,  2.4532e-02, -6.1000e-03,\n",
       "         -2.3796e-02, -7.2089e-02,  8.3061e-01, -9.1917e-01,  1.0840e-01,\n",
       "         -7.3639e-01,  7.4211e-02, -5.5182e-01, -3.0707e-03, -1.6027e-02,\n",
       "         -1.4208e-02,  7.0403e-01, -1.4441e-01,  7.7621e-01,  6.1497e-02,\n",
       "          3.8939e-02, -4.3359e-02, -3.8957e-02,  8.4656e-02,  7.0245e-01,\n",
       "         -8.6755e-02,  1.7550e-02,  7.1846e-01, -7.1069e-02,  6.3390e-02,\n",
       "          7.5840e-03, -7.7019e-01, -2.2752e-02, -8.3622e-02, -5.8121e-02,\n",
       "         -2.6829e-03,  6.6461e-02, -4.3967e-02,  1.1675e-02, -3.3203e-01,\n",
       "          1.1996e-01, -1.0619e-01,  9.2276e-02, -1.3400e-02,  9.4112e-01,\n",
       "         -2.2573e-02,  1.6751e-01, -3.4885e-01, -2.4326e-01, -3.2522e-02,\n",
       "         -8.9840e-03, -5.3414e-02,  9.4256e-01, -2.2938e-01, -9.8753e-01,\n",
       "         -4.7231e-01, -4.1518e-02, -5.0285e-02, -8.4463e-02,  2.6219e-02,\n",
       "         -2.6515e-03,  9.3843e-01, -9.5286e-01,  2.8301e-02,  7.1728e-02,\n",
       "         -8.0114e-01, -6.7113e-02, -1.2409e-01,  4.9315e-01, -2.2957e-01,\n",
       "          2.5472e-01, -8.7463e-01, -5.9800e-02,  1.4178e-02,  5.0370e-02,\n",
       "          7.9464e-01,  3.6470e-02, -3.7170e-01, -6.2081e-02, -9.5654e-01,\n",
       "          5.2961e-02, -1.4652e-03,  9.8122e-01,  1.1055e-02,  9.0783e-01,\n",
       "          3.0337e-02,  4.5659e-02,  5.5119e-02,  4.0692e-02, -1.1038e-01,\n",
       "          7.2512e-02,  9.5621e-01,  5.7147e-02,  9.3765e-01, -6.4747e-01,\n",
       "          1.1507e-01, -5.4302e-02, -1.1085e-02,  1.9802e-03, -3.7108e-01,\n",
       "          1.8446e-02, -1.4383e-02,  7.7695e-01, -1.9408e-01,  2.5246e-02,\n",
       "          6.7948e-02, -5.2251e-02,  9.0529e-02, -6.8763e-01, -1.9925e-01,\n",
       "          2.6744e-01,  3.1028e-02,  5.6100e-02, -1.0659e-01, -5.1481e-02,\n",
       "         -2.5195e-02,  4.6012e-01, -7.2329e-01,  1.7849e-03, -1.7189e-02,\n",
       "          2.5362e-02, -9.1316e-03, -9.5487e-02,  7.2232e-02, -7.8103e-02,\n",
       "          6.6101e-01,  8.8435e-02,  8.7688e-01, -1.6032e-02, -5.5757e-02,\n",
       "         -4.4755e-02,  2.0050e-01,  9.6662e-01,  2.9673e-01, -3.2007e-02,\n",
       "         -1.7923e-02, -1.6440e-01, -1.0461e-02,  9.0765e-02, -7.2544e-01,\n",
       "          2.2716e-02,  5.8215e-01,  7.7231e-01,  1.5561e-01, -6.3538e-02,\n",
       "          8.6609e-02, -3.9603e-02, -9.2758e-02,  7.0084e-01, -8.1104e-01,\n",
       "         -2.8787e-03,  2.9028e-02,  6.8262e-03, -9.5778e-02,  1.8937e-01,\n",
       "         -9.1978e-03, -4.4025e-02,  2.1104e-03, -2.9746e-02,  4.8875e-02,\n",
       "         -8.2450e-02, -8.4538e-01, -1.8455e-02,  1.6669e-02, -3.3967e-02,\n",
       "         -4.8767e-02,  6.3331e-02,  3.1946e-01, -4.5818e-01, -6.0135e-01,\n",
       "          5.8191e-03, -8.4450e-02, -8.3773e-02,  1.0661e-02, -2.5877e-01,\n",
       "          4.2335e-02, -1.6189e-02, -1.2251e-01, -5.9511e-02, -9.9256e-01,\n",
       "          6.7811e-01,  5.7140e-02,  8.6922e-01,  8.3695e-03, -3.9466e-02,\n",
       "         -1.0140e-01,  1.5928e-02, -3.0777e-02,  9.9428e-02, -4.7455e-02,\n",
       "         -1.0428e-01,  5.6803e-01, -7.7033e-01, -8.7887e-02,  8.4342e-02,\n",
       "          1.0579e-01,  4.2812e-02,  3.5396e-01, -8.8919e-01, -5.8955e-02,\n",
       "         -3.3511e-02, -9.8312e-01, -2.1719e-03, -3.0386e-02, -2.3034e-02,\n",
       "         -9.6272e-01, -7.1841e-01, -2.4703e-02,  4.9625e-02,  8.0735e-02,\n",
       "         -5.8401e-03, -8.6553e-01, -3.7507e-01,  7.0127e-02,  4.7306e-02,\n",
       "          7.5674e-01, -6.3537e-02, -3.5351e-01, -7.6495e-01, -2.3481e-02,\n",
       "          6.4433e-02, -3.5318e-02,  7.2928e-02, -2.9695e-02, -2.9569e-02,\n",
       "         -9.0372e-01, -7.3230e-01,  5.6127e-01, -2.0525e-01,  5.6624e-03,\n",
       "          7.0177e-01, -1.0611e-02,  5.6730e-02, -2.5405e-02, -5.7643e-02,\n",
       "         -1.3004e-03,  2.0148e-02,  6.9575e-02,  9.8088e-01, -4.8290e-01,\n",
       "         -2.4505e-01,  6.4069e-01,  3.5834e-02,  5.3850e-01, -6.3125e-03,\n",
       "          3.7808e-02,  9.4113e-03, -9.6656e-01, -6.1935e-02, -3.2024e-02,\n",
       "          1.3461e-01, -1.0385e-01,  6.7394e-02, -2.7822e-02,  3.2751e-02,\n",
       "         -6.0791e-02, -2.6617e-01,  1.9136e-02, -9.8094e-01,  8.2742e-01,\n",
       "          9.7904e-03,  6.5610e-02,  3.1748e-01,  7.9915e-02,  1.4188e-01,\n",
       "         -9.1264e-01,  3.4060e-02,  5.3422e-01,  5.2498e-01,  4.0564e-01,\n",
       "          2.5352e-02,  5.3092e-01,  1.1313e-01,  8.8759e-01,  1.5933e-01,\n",
       "         -1.5242e-02,  4.0498e-02,  1.1865e-01, -7.8255e-01,  2.6234e-01,\n",
       "         -6.4587e-02,  9.8180e-02,  4.2388e-01,  5.3466e-01,  1.5576e-02,\n",
       "         -3.4462e-01, -3.6342e-02, -9.9222e-01,  2.6462e-02, -1.7844e-02,\n",
       "         -1.8508e-02, -3.3888e-02,  4.7701e-01, -3.2934e-02,  8.5054e-01,\n",
       "          5.0533e-02,  3.9112e-02,  1.9157e-02, -5.2996e-02,  6.9766e-02,\n",
       "         -1.7547e-01,  1.0452e-01,  2.4136e-01,  8.5899e-02, -4.6926e-01,\n",
       "         -1.2859e-02, -3.3742e-03, -5.9571e-02,  5.2029e-01,  2.2823e-02,\n",
       "         -1.7813e-01, -1.6251e-02,  3.6693e-02,  8.3174e-01,  1.6724e-02,\n",
       "          7.4243e-01, -8.9944e-01, -9.7980e-01,  8.3304e-02, -5.8330e-01,\n",
       "         -8.9489e-01,  5.8258e-02,  1.6749e-02,  6.7604e-02,  6.4020e-01,\n",
       "         -7.7131e-02,  4.1331e-02, -1.7718e-02,  9.8991e-03,  3.0405e-01,\n",
       "          9.2361e-03, -4.6998e-01,  9.6490e-02,  5.5990e-02,  4.0410e-01,\n",
       "          8.5209e-01, -4.0586e-02,  4.2684e-03,  3.4308e-01,  5.7150e-02,\n",
       "         -4.0315e-01,  1.5685e-01,  1.4423e-01,  8.2139e-01,  5.0485e-03,\n",
       "          3.7153e-02,  3.5112e-01, -9.0965e-02,  6.5241e-01, -9.7625e-01,\n",
       "          3.8870e-01,  5.1834e-02,  1.1936e-02,  2.5641e-02, -1.8071e-02,\n",
       "          4.1174e-02,  8.3883e-02,  1.5772e-02, -4.6164e-03,  2.3161e-03,\n",
       "          9.6021e-01, -1.8512e-03,  2.6064e-02,  9.4292e-02,  7.2463e-02,\n",
       "         -9.9356e-01,  3.2726e-02,  7.7338e-02, -3.4337e-02, -1.8215e-01,\n",
       "          9.2575e-01, -1.4561e-01,  5.7702e-02, -5.2001e-02,  6.8672e-01,\n",
       "         -2.8794e-02,  8.1420e-01,  4.2873e-02, -1.7895e-01,  1.0383e-02,\n",
       "          1.2582e-02,  6.6852e-01, -2.2966e-02, -4.0961e-01, -4.1761e-01,\n",
       "         -5.6840e-02,  8.0009e-01,  7.8560e-01, -4.2799e-01, -2.0483e-03,\n",
       "         -7.6418e-01,  3.8926e-02, -4.2867e-01,  1.6579e-01,  4.0029e-02,\n",
       "         -1.6265e-01, -2.3516e-02, -4.3505e-03, -2.0928e-02, -4.3739e-01,\n",
       "          7.9277e-02, -1.3300e-01, -7.9514e-02, -8.3499e-03, -5.6553e-01,\n",
       "          3.4780e-02, -2.2811e-02, -5.1934e-02, -8.0211e-01,  3.6126e-02,\n",
       "         -8.6311e-02,  5.0001e-03, -6.9568e-01,  4.3481e-02, -5.4120e-02,\n",
       "          2.3620e-02, -8.6745e-01, -3.0457e-02,  3.1502e-02,  3.1419e-01,\n",
       "          5.4790e-02, -6.5714e-03,  4.0167e-01, -4.6072e-03, -9.6058e-02,\n",
       "          7.8705e-01, -2.4285e-02, -2.6422e-01,  3.9268e-02, -2.0103e-02,\n",
       "         -4.9671e-02,  9.3041e-01,  2.4287e-02,  7.1054e-01, -3.5962e-02,\n",
       "          1.4262e-02,  7.5571e-02, -6.0516e-01,  9.1194e-02,  7.0879e-01,\n",
       "          2.9994e-02, -9.9445e-01, -9.5929e-01,  5.6835e-02,  7.5690e-02,\n",
       "         -3.3472e-01, -8.2237e-01, -3.0703e-02, -4.9088e-02, -4.5864e-02,\n",
       "          3.2620e-02, -1.2731e-02,  2.7750e-01, -1.7114e-02, -6.9890e-01,\n",
       "         -9.1533e-02,  6.7375e-03, -2.2730e-02, -3.5960e-02,  1.0577e-01,\n",
       "         -9.2285e-03, -1.0035e-01,  9.5769e-01, -9.1952e-01, -9.8741e-01,\n",
       "         -1.4856e-01, -4.8754e-03, -7.6239e-01, -4.4895e-02,  9.0825e-03,\n",
       "         -1.1628e-02,  5.9776e-01,  6.1495e-02, -8.1022e-01,  8.7328e-01,\n",
       "          3.3119e-01, -3.1449e-01,  1.5403e-02, -7.5830e-01,  5.9020e-02,\n",
       "         -8.4368e-01,  3.3864e-02, -4.2820e-01, -4.6485e-02, -4.3011e-02,\n",
       "         -3.3130e-02,  3.8292e-01, -1.6196e-02,  4.4562e-02,  7.6444e-02,\n",
       "         -2.0097e-01, -5.2445e-01, -3.0913e-02,  7.2905e-01,  3.1383e-02,\n",
       "          5.0120e-01, -3.0682e-01, -9.4162e-03,  1.2386e-02,  9.8416e-01,\n",
       "          9.8227e-01, -9.6113e-01,  1.9098e-02,  4.0591e-02, -1.4901e-02,\n",
       "         -7.1216e-01,  6.0432e-02, -2.0367e-01, -3.7658e-03, -9.2425e-02,\n",
       "          1.7859e-02,  1.1243e-01, -3.8568e-01,  3.8190e-01,  6.4977e-02,\n",
       "          6.6152e-02,  4.9542e-01, -3.1318e-02, -1.1610e-01,  1.1246e-01,\n",
       "          2.0942e-02,  5.0184e-02, -7.6474e-01,  5.5843e-01, -3.4645e-01,\n",
       "          9.7822e-01,  4.7621e-02,  3.4887e-03, -2.9863e-01,  9.0369e-01,\n",
       "         -1.9583e-02, -5.9761e-02, -9.7793e-01, -6.7303e-02, -5.5084e-01,\n",
       "          9.7273e-01, -3.7803e-01,  7.7860e-01,  5.9926e-03,  6.5147e-01,\n",
       "          4.3530e-01,  5.3162e-01, -2.0605e-01, -9.0792e-01,  2.8719e-01,\n",
       "         -8.4723e-03,  2.7934e-02, -5.1838e-01, -1.2247e-01,  2.4741e-02,\n",
       "         -7.9397e-01,  8.6099e-02, -7.2459e-02,  4.2247e-02, -6.5370e-02,\n",
       "         -1.0505e-01,  1.0139e-01,  8.0226e-02, -2.5885e-01,  1.1011e-01,\n",
       "          3.3219e-02, -1.3653e-02,  6.8186e-01, -1.2432e-01, -6.1911e-01,\n",
       "         -1.0915e-01, -7.6401e-02, -9.8306e-01,  4.1433e-02,  3.8849e-02,\n",
       "         -5.2257e-02,  2.1864e-01,  3.7489e-02,  2.9452e-02, -6.1349e-01,\n",
       "          2.4707e-02,  5.0279e-02, -9.7844e-02, -2.3465e-02, -1.1745e-01,\n",
       "          4.5680e-01,  4.9275e-01, -3.6432e-02, -2.7654e-01,  8.6255e-01,\n",
       "          5.9526e-02,  1.2261e-01,  4.8693e-01, -6.9193e-01, -8.1380e-01,\n",
       "         -5.2388e-01,  2.9877e-02, -1.4240e-01, -4.6595e-02, -8.1355e-01,\n",
       "          1.4989e-01, -1.6468e-01,  3.7357e-01, -1.6967e-02,  5.1724e-01,\n",
       "          5.6011e-02, -6.9065e-02,  1.4471e-02]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "text = '여기부터 시작합니다.'\n",
    "inputs = tokenizer.tokenize([text])\n",
    "out = model(input_ids = torch.tensor(inputs['input_ids']),\n",
    "            attention_mask = torch.tensor(inputs['attention_mask']))\n",
    "out.pooler_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingFace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
